{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing for CLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_file_path = \"../data/p8_ee_tt_ecm365/root\"\n",
    "parquet_file_path = \"../data/p8_ee_tt_ecm365/parquet\"\n",
    "module_path = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/p8_ee_tt_ecm365/root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import lmdb\n",
    "import random\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import vector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_path)\n",
    "from src.datasets.utils import Collater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "These are defined in `data_processing/cld_processing` but are reproduced here for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "pion_mass = 0.13957\n",
    "B = -2.0  # magnetic field in T (-2 for CLD FCC-ee)\n",
    "c = 3e8  # speed of light in m/s\n",
    "scale = 1000\n",
    "\n",
    "from data_processing.cld_processing import (\n",
    "    get_event_data,\n",
    "    gen_to_features,\n",
    "    track_to_features,\n",
    "    cluster_to_features,\n",
    "    process_calo_hit_data,\n",
    "    process_tracker_hit_data,\n",
    "    create_track_to_hit_coo_matrix,\n",
    "    create_cluster_to_hit_coo_matrix,\n",
    "    genparticle_track_adj,\n",
    "    create_genparticle_to_genparticle_coo_matrix,\n",
    "    create_genparticle_to_genparticle_coo_matrix2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to extract:\n",
    "- [x] hits (all tracker and calo hit features in the event)\n",
    "- [x] genparticles (all genparticle features in the event, noting that the genparticles are in the form of a decay tree)\n",
    "- [x] genparticle_to_genparticle (sparse association matrix for the genparticle decay tree)\n",
    "- [x] hits_to_genparticles (sparse association matrix)\n",
    "- [x] tracks (all track features in the event)\n",
    "- [x] tracks_to_hits (sparse association matrix between tracks and tracker hits)\n",
    "- [x] clusters (all cluster features in the event)\n",
    "- [x] clusters_to_hits (sparse association matrix between clusters and calo hits)\n",
    "\n",
    "Additional data to extract:\n",
    "- [x] genparticle_to_track (sparse association matrix for the genparticle to track associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files_dir = Path(root_file_path)\n",
    "root_file = root_files_dir / \"reco_p8_ee_tt_ecm365_60000.root\"\n",
    "fi = uproot.open(root_file)\n",
    "ev = fi[\"events\"]\n",
    "\n",
    "# which event to pick from the file\n",
    "iev = 2\n",
    "\n",
    "collectionIDs = {\n",
    "    k: v\n",
    "    for k, v in zip(\n",
    "        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "        fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\"events___idTable/m_collectionIDs\"][0],\n",
    "    )\n",
    "}\n",
    "\n",
    "event_data = get_event_data(ev)\n",
    "\n",
    "# Extract gparticle, track, and cluster features\n",
    "gen_features = gen_to_features(event_data, iev)\n",
    "track_features = track_to_features(event_data, iev)\n",
    "cluster_features = cluster_to_features(\n",
    "    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    ")\n",
    "\n",
    "# Process calorimeter hit data\n",
    "calo_hit_features, genparticle_to_calo_hit_matrix, calo_hit_idx_local_to_global = process_calo_hit_data(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "\n",
    "# Process tracker hit data\n",
    "tracker_hit_features, genparticle_to_tracker_hit_matrix, tracker_hit_idx_local_to_global = process_tracker_hit_data(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "\n",
    "# Create the track-to-trackerhit adjacency matrix\n",
    "track_to_tracker_hit_matrix, tracker_hit_idx_local_to_global_2 = create_track_to_hit_coo_matrix(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "assert (\n",
    "    tracker_hit_idx_local_to_global == tracker_hit_idx_local_to_global_2\n",
    "), \"Local to global tracker hit index mapping mismatch!\"\n",
    "\n",
    "# Ceate the cluster-to-clusterhit adjacency matrix\n",
    "cluster_to_cluster_hit_matrix, calo_hit_idx_local_to_global_2 = create_cluster_to_hit_coo_matrix(\n",
    "    event_data, iev, collectionIDs\n",
    ")\n",
    "assert (\n",
    "    calo_hit_idx_local_to_global == calo_hit_idx_local_to_global_2\n",
    "), \"Local to global calorimeter hit index mapping mismatch!\"\n",
    "\n",
    "# Create the genparticle-to-track adjacency matrix\n",
    "gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "\n",
    "# Create the genparticle-to-genparticle adjacency matrix\n",
    "gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "# Check consistency between the two methods gp-to-gp methods\n",
    "gp_to_gp2 = create_genparticle_to_genparticle_coo_matrix2(event_data, iev)\n",
    "\n",
    "# Get the number of genparticles in the event\n",
    "n_gp = len(ev[\"MCParticles.momentum.x\"].array()[iev])\n",
    "\n",
    "# create coo matrix through the COOs in gp_to_gp and gp_to_gp2\n",
    "coo_matrix_gp_to_gp = coo_matrix(\n",
    "    (gp_to_gp[\"weight\"], (gp_to_gp[\"parent_idx\"], gp_to_gp[\"daughter_idx\"])), shape=(n_gp, n_gp)\n",
    ")\n",
    "coo_matrix_gp_to_gp2 = coo_matrix(\n",
    "    (gp_to_gp2[\"weight\"], (gp_to_gp2[\"parent_idx\"], gp_to_gp2[\"daughter_idx\"])), shape=(n_gp, n_gp)\n",
    ")\n",
    "# Check if the two dense gp_to_gp matrices are equal\n",
    "assert (coo_matrix_gp_to_gp.todense() == coo_matrix_gp_to_gp2.todense()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gen_features), type(track_features), type(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gen_features[\"px\"]), type(track_features[\"px\"]), type(cluster_features[\"position.x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(calo_hit_features), type(tracker_hit_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(calo_hit_features[\"energy\"]), type(tracker_hit_features[\"energy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix),\n",
    "    type(genparticle_to_tracker_hit_matrix),\n",
    "    type(track_to_tracker_hit_matrix),\n",
    "    type(cluster_to_cluster_hit_matrix),\n",
    "    type(gp_to_gp),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    type(genparticle_to_calo_hit_matrix[\"hit_idx\"]),\n",
    "    type(genparticle_to_tracker_hit_matrix[\"gen_idx\"]),\n",
    "    type(track_to_tracker_hit_matrix[\"track_idx\"]),\n",
    "    type(cluster_to_cluster_hit_matrix[\"cluster_idx\"]),\n",
    "    type(gp_to_gp[\"parent_idx\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from calo_hit_features and tracker_hit_features into a single dataframe\n",
    "df = pandas.DataFrame()\n",
    "\n",
    "# Extract data from calo_hit_features and tracker_hit_features using numpy.concatenate\n",
    "df[\"px\"] = np.concatenate([calo_hit_features[\"position.x\"], tracker_hit_features[\"position.x\"]])\n",
    "df[\"py\"] = np.concatenate([calo_hit_features[\"position.y\"], tracker_hit_features[\"position.y\"]])\n",
    "df[\"pz\"] = np.concatenate([calo_hit_features[\"position.z\"], tracker_hit_features[\"position.z\"]])\n",
    "df[\"energy\"] = np.concatenate([1000 * calo_hit_features[\"energy\"], 1000 * tracker_hit_features[\"energy\"]])\n",
    "df[\"plotsize\"] = 0.0\n",
    "df[\"subdetector\"] = np.concatenate([calo_hit_features[\"subdetector\"], tracker_hit_features[\"subdetector\"]])\n",
    "\n",
    "# Calculate plotsize based on subdetector values\n",
    "df.loc[df[\"subdetector\"] == 0, \"plotsize\"] = df.loc[df[\"subdetector\"] == 0, \"energy\"] / 5.0\n",
    "df.loc[df[\"subdetector\"] == 1, \"plotsize\"] = df.loc[df[\"subdetector\"] == 1, \"energy\"] / 10.0\n",
    "df.loc[df[\"subdetector\"] == 2, \"plotsize\"] = df.loc[df[\"subdetector\"] == 2, \"energy\"] * 100.0\n",
    "df.loc[df[\"subdetector\"] == 3, \"plotsize\"] = df.loc[df[\"subdetector\"] == 3, \"energy\"] * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks\n",
    "def helix_eq(charge, bfield, v):\n",
    "    \"\"\"Calculate the 3D helical trajectory of a charged particle in a magnetic field.\n",
    "\n",
    "    This function computes the x, y, and z coordinates of a charged particle's\n",
    "    helical trajectory in a uniform magnetic field over a range of time values.\n",
    "\n",
    "    Args:\n",
    "        charge (float): The charge of the particle in elementary charge units.\n",
    "        bfield (float): The magnetic field strength in Tesla.\n",
    "        v (vector): A vector object representing the particle's velocity and properties.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of three lists (x, y, z) representing the particle's\n",
    "        trajectory in 3D space:\n",
    "            - x (list): x-coordinates of the trajectory.\n",
    "            - y (list): y-coordinates of the trajectory.\n",
    "            - z (list): z-coordinates of the trajectory.\n",
    "    \"\"\"\n",
    "    R = v.pt / (charge * 0.3 * bfield)\n",
    "    omega = charge * 0.3 * bfield / (v.gamma * v.mass)\n",
    "    t_values = np.linspace(0, 2 / (c * v.beta), 10)\n",
    "    x = list(scale * R * np.cos(omega * c * t_values + v.phi - np.pi / 2) - scale * R * np.cos(v.phi - np.pi / 2))\n",
    "    y = list(scale * R * np.sin(omega * c * t_values + v.phi - np.pi / 2) - scale * R * np.sin(v.phi - np.pi / 2))\n",
    "    z = list(scale * v.pz * c * t_values / (v.gamma * v.mass))\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "track_px, track_py, track_pz, track_charge = (\n",
    "    track_features[\"px\"],\n",
    "    track_features[\"py\"],\n",
    "    track_features[\"pz\"],\n",
    "    track_features[\"q\"],\n",
    ")\n",
    "\n",
    "track_mass = np.zeros_like(track_px) + pion_mass\n",
    "\n",
    "track_x, track_y, track_z = [], [], []\n",
    "for irow in range(len(track_px)):\n",
    "\n",
    "    # convert to vector\n",
    "    v = vector.obj(px=track_px[irow], py=track_py[irow], pz=track_pz[irow], mass=track_mass[irow])\n",
    "\n",
    "    x, y, z = helix_eq(track_charge[irow], B, v)\n",
    "    track_x += x\n",
    "    track_y += y\n",
    "    track_z += z\n",
    "\n",
    "    track_x += [None]  # Add None to separate tracks in the plot\n",
    "    track_y += [None]\n",
    "    track_z += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"Raw ECAL hit\", 1: \"Raw HCAL hit\", 2: \"Raw Muon chamber hit\", 3: \"Raw tracker hit\"}\n",
    "\n",
    "subdetector_color = {0: \"steelblue\", 1: \"green\", 2: \"orange\", 3: \"red\"}  # ECAL  # HCAL  # MUON  # Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "# raw hits\n",
    "for subdetector in [0, 1, 2, 3]:\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=np.clip(df[\"px\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        y=np.clip(df[\"py\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        z=np.clip(df[\"pz\"][df[\"subdetector\"] == subdetector], -4000, 4000),\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=np.clip(2 + 2 * np.log(df[\"plotsize\"]), 1, 15),\n",
    "            color=subdetector_color[subdetector],\n",
    "            colorscale=\"Viridis\",\n",
    "            opacity=0.8,\n",
    "        ),\n",
    "        name=labels[subdetector],\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# add the tracks\n",
    "trace = go.Scatter3d(\n",
    "    x=np.array(track_x),\n",
    "    y=np.array(track_y),\n",
    "    z=np.array(track_z),\n",
    "    mode=\"lines\",\n",
    "    name=\"Track\",\n",
    "    line=dict(color=\"red\"),\n",
    ")\n",
    "traces.append(trace)\n",
    "\n",
    "# # add the clusters\n",
    "trace = go.Scatter3d(\n",
    "    x=cluster_features[\"position.x\"],\n",
    "    y=cluster_features[\"position.y\"],\n",
    "    z=cluster_features[\"position.z\"],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=cluster_features[\"energy\"],\n",
    "        color=\"blue\",\n",
    "        opacity=0.8,\n",
    "    ),\n",
    "    name=\"ECAL/HCAL clusters\",\n",
    ")\n",
    "traces.append(trace)\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"\", showticklabels=False),\n",
    "        yaxis=dict(title=\"\", showticklabels=False),\n",
    "        zaxis=dict(title=\"\", showticklabels=False),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.0),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    legend=dict(x=0.8, y=0.5, font=dict(size=16)),  # https://plotly.com/python/legend/\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.update_traces(\n",
    "    marker_line_width=0, selector=dict(type=\"scatter3d\")\n",
    ")  # for plotly to avoid plotting white spots when things overlap\n",
    "# fig.write_image(\"pic_tracks_rawcalohits.pdf\", width=1000, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg_dict = {\n",
    "    22: \"photon\",  # photon\n",
    "    11: \"electron\",  # electron\n",
    "    13: \"muon\",  # muon\n",
    "    130: \"n. hadron\",  # neutral hadron\n",
    "    211: \"ch. hadron\",  # charged hadron\n",
    "}\n",
    "\n",
    "color_dict = {\n",
    "    \"photon\": \"red\",  # photon\n",
    "    \"electron\": \"green\",  # electron\n",
    "    \"muon\": \"purple\",  # muon\n",
    "    \"n. hadron\": \"orange\",  # neutral hadron\n",
    "    \"ch. hadron\": \"blue\",  # charged hadron\n",
    "    None: \"black\",  # placeholder for skipped element\n",
    "}\n",
    "\n",
    "# Extract relevant data from gen_features\n",
    "gen_px = gen_features[\"px\"]\n",
    "gen_py = gen_features[\"py\"]\n",
    "gen_pz = gen_features[\"pz\"]\n",
    "gen_mass = gen_features[\"mass\"]\n",
    "gen_charge = gen_features[\"charge\"]\n",
    "gen_pdg = ak.to_numpy(np.absolute(gen_features[\"PDG\"]))\n",
    "\n",
    "# Set all other particles to ch.had or n.had\n",
    "gen_pdg[(gen_pdg != 13) & (gen_pdg != 11) & (gen_pdg != 22) & ak.to_numpy(np.abs(gen_charge) > 0)] = (\n",
    "    211  # when not filtering genstatus==1, charge can be between 0 and 1\n",
    ")\n",
    "gen_pdg[(gen_pdg != 13) & (gen_pdg != 11) & (gen_pdg != 22) & ak.to_numpy(np.abs(gen_charge) == 0)] = 130\n",
    "\n",
    "# Extrapolate MC particle trajectories\n",
    "mc_x = []\n",
    "mc_y = []\n",
    "mc_z = []\n",
    "pdg_list = []\n",
    "for irow in range(len(gen_px)):\n",
    "\n",
    "    # Convert to vector\n",
    "    v = vector.obj(px=gen_px[irow], py=gen_py[irow], pz=gen_pz[irow], mass=gen_mass[irow])\n",
    "    if gen_charge[irow] == 0:\n",
    "        this_mc_x = [0, np.clip(scale * v.px / v.mag, -4000, 4000)]\n",
    "        this_mc_y = [0, np.clip(scale * v.py / v.mag, -4000, 4000)]\n",
    "        this_mc_z = [0, np.clip(scale * v.pz / v.mag, -4000, 4000)]\n",
    "    else:\n",
    "        x, y, z = helix_eq(gen_charge[irow], B, v)\n",
    "        this_mc_x = x\n",
    "        this_mc_y = y\n",
    "        this_mc_z = z\n",
    "\n",
    "    pdg_list += len(this_mc_x) * [pdg_dict[gen_pdg[irow]]]\n",
    "\n",
    "    mc_x += this_mc_x\n",
    "    mc_y += this_mc_y\n",
    "    mc_z += this_mc_z\n",
    "\n",
    "    mc_x += [None]\n",
    "    mc_y += [None]\n",
    "    mc_z += [None]\n",
    "    pdg_list += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D scatter plot with one trace per particle\n",
    "traces = []\n",
    "unique_particles = set(pdg_list)  # Get unique particle types\n",
    "\n",
    "for particle in unique_particles:\n",
    "\n",
    "    # Get indices for the current particle, including None at the end of each track\n",
    "    indices = []\n",
    "    for i, p in enumerate(pdg_list):\n",
    "        if p is None:  # we don't need to add non-particles\n",
    "            continue\n",
    "        if p == particle:\n",
    "            indices.append(i)\n",
    "            if pdg_list[i + 1] is None:\n",
    "                indices.append(i + 1)  # Add None at the end of the track to separate tracks in plot\n",
    "\n",
    "    # Create a separate trace for each particle\n",
    "    traces.append(\n",
    "        go.Scatter3d(\n",
    "            x=np.array(mc_x)[indices],\n",
    "            y=np.array(mc_y)[indices],\n",
    "            z=np.array(mc_z)[indices],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color_dict[particle]),  # Assign color for the particle\n",
    "            name=f\"{particle}\",  # Add particle name to the legend\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the axis names\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"\", showticklabels=False),\n",
    "        yaxis=dict(title=\"\", showticklabels=False),\n",
    "        zaxis=dict(title=\"\", showticklabels=False),\n",
    "        camera=dict(\n",
    "            up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "            center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "            eye=dict(x=0, y=0, z=2.0),  # Sets the position of the camera\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# Create the figure and display the plot\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.update_traces(\n",
    "    marker_line_width=0, selector=dict(type=\"scatter3d\")\n",
    ")  # Avoid plotting white spots when things overlap\n",
    "# fig.write_image(\"pic_particles_legend.pdf\", width=1000, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the alignment of extrapolated tracks and their associated hits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))  # Adjust figsize as needed\n",
    "axes = axes.ravel()\n",
    "\n",
    "for itrk in range(9):\n",
    "    plt.sca(axes[itrk])\n",
    "    v = vector.obj(px=track_px[itrk], py=track_py[itrk], pz=track_pz[itrk], mass=track_mass[itrk])\n",
    "\n",
    "    # Get the global hit indices associated with the current track\n",
    "    # track_to_tracker_hit_matrix is a tuple where the first element contains the track indices and the second element contains the hit indices\n",
    "    hit_indices = track_to_tracker_hit_matrix[\"hit_idx\"][track_to_tracker_hit_matrix[\"track_idx\"] == itrk]\n",
    "\n",
    "    # Extract the corresponding hit positions\n",
    "    hs_x = tracker_hit_features[\"position.x\"][hit_indices]\n",
    "    hs_z = tracker_hit_features[\"position.z\"][hit_indices]\n",
    "\n",
    "    # Calculate the helix trajectory\n",
    "    x, y, z = helix_eq(track_charge[itrk], B, v)\n",
    "\n",
    "    # Plot the track and associated hits\n",
    "    plt.plot(x, z, label=\"Track\")\n",
    "    plt.scatter(hs_x, hs_z, color=\"red\", marker=\".\", label=\"Tracker hit\")\n",
    "    plt.xlim(-2000, 2000)\n",
    "    plt.ylim(-2000, 2000)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    axes[itrk].set_box_aspect(1)\n",
    "\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the alignment of clusters and their associated hits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))  # Adjust figsize as needed\n",
    "axes = axes.ravel()\n",
    "\n",
    "for icls in range(9):\n",
    "    plt.sca(axes[icls])\n",
    "\n",
    "    # Get the global hit indices associated with the current cluster\n",
    "    # cluster_to_cluster_hit_matrix is a tuple where the first element contains the cluster indices and the second element contains the hit indices\n",
    "    hit_indices = cluster_to_cluster_hit_matrix[\"hit_idx\"][cluster_to_cluster_hit_matrix[\"cluster_idx\"] == icls]\n",
    "\n",
    "    # Extract the corresponding hit positions\n",
    "    hs_x = calo_hit_features[\"position.x\"][hit_indices]\n",
    "    hs_y = calo_hit_features[\"position.y\"][hit_indices]\n",
    "\n",
    "    # Plot the cluster and associated hits\n",
    "    plt.scatter(\n",
    "        cluster_features[\"position.x\"][icls],\n",
    "        cluster_features[\"position.y\"][icls],\n",
    "        s=100 * cluster_features[\"energy\"][icls],\n",
    "        alpha=0.5,\n",
    "        label=\"cluster\",\n",
    "    )\n",
    "    plt.scatter(hs_x, hs_y, color=\"red\", marker=\".\", label=\"hit\")\n",
    "    plt.xlim(-4000, 4000)\n",
    "    plt.ylim(-4000, 4000)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    axes[icls].set_box_aspect(1)\n",
    "\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genparticle to calorimeter hit associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract genparticle_to_calo_hit_matrix from event_data1\n",
    "# This matrix contains the mapping of genparticles to calorimeter hits in a COO format\n",
    "rows = genparticle_to_calo_hit_matrix[\"gen_idx\"]\n",
    "cols = genparticle_to_calo_hit_matrix[\"hit_idx\"]\n",
    "weights = genparticle_to_calo_hit_matrix[\"weight\"]\n",
    "\n",
    "# create dense coo amtrix\n",
    "gp_to_calo_hit_matrix = coo_matrix((weights, (rows, cols)), shape=(np.max(rows) + 1, np.max(cols) + 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights), np.sum(gp_to_calo_hit_matrix > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many genparticles leave at least one hit? (we haven't filtered on generatorStatus yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rows that sum to greater than 0\n",
    "non_zero_rows = np.where(np.sum(gp_to_calo_hit_matrix, axis=1) > 0)[0]\n",
    "\n",
    "# Create a new matrix with the extracted rows\n",
    "gp_to_calo_hit_matrix_non_zero = gp_to_calo_hit_matrix[non_zero_rows, :]\n",
    "gp_to_calo_hit_matrix.shape, gp_to_calo_hit_matrix_non_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of genparticles: {gp_to_calo_hit_matrix.shape[0]}\")\n",
    "print(f\"Number of genparticles with > 0 hits: {len(non_zero_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many genparticles leave more than 1 hit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows with more than 1 element with weight > 0\n",
    "rows_with_multiple_elements = np.sum(np.sum(gp_to_calo_hit_matrix > 0, axis=1) > 1)\n",
    "print(f\"Number of genparticles leaving more than 1 hit: {rows_with_multiple_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hits are assoicated to more than one genparticle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count cols with more than one element > 0\n",
    "cols_with_multiple_elements = np.sum(np.sum(gp_to_calo_hit_matrix > 0, axis=0) > 1)\n",
    "\n",
    "# Extract rows that sum to greater than 0\n",
    "multiple_gp_per_hit_mask = np.where(np.sum(gp_to_calo_hit_matrix > 0, axis=0) > 1)\n",
    "\n",
    "# Create a new matrix with the extracted rows\n",
    "multiple_gp_per_hit = gp_to_calo_hit_matrix[:, multiple_gp_per_hit_mask[1]]\n",
    "multiple_gp_per_hit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of hits with multiple genparticles: {multiple_gp_per_hit.shape[1]}\")\n",
    "print(f\"Number of hits with multiple genparticles: {cols_with_multiple_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of the hits associated to multiple genparticles, how many genparticles are each hit associated to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(multiple_gp_per_hit > 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_hit in range(multiple_gp_per_hit.shape[1]):\n",
    "    gp_links = multiple_gp_per_hit[:, i_hit]\n",
    "    gp_link_mask = gp_links > 0\n",
    "    gp_links = gp_links[gp_link_mask]\n",
    "    print(f\"Hit {i_hit} is linked to genparticles: {np.where(gp_link_mask)[0]} with weights {gp_links}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_root_files_to_parquet(input_dir, output_dir, max_root_files=None):\n",
    "    \"\"\"\n",
    "    Process ROOT files and save extracted features and matrices into Parquet files using ak arrays.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str or Path): Directory containing ROOT files.\n",
    "        output_dir (str or Path): Directory to save Parquet files.\n",
    "        max_root_files (int, optional): Maximum number of ROOT files to process. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    root_counter = 0\n",
    "    root_file_list = sorted(list(Path(input_dir).rglob(\"*.root\")))[:max_root_files]\n",
    "    total_files_to_process = len(root_file_list)\n",
    "\n",
    "    for root_file in tqdm(root_file_list, desc=\"Processing ROOT files\", total=total_files_to_process):\n",
    "        try:\n",
    "            output_file = output_dir / f\"{root_file.stem}.parquet\"\n",
    "            if output_file.exists():\n",
    "                print(f\"Output file {output_file} already exists. Skipping processing.\")\n",
    "                return\n",
    "\n",
    "            fi = uproot.open(root_file)\n",
    "            collectionIDs = {\n",
    "                k: v\n",
    "                for k, v in zip(\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\n",
    "                        \"events___idTable/m_collectionIDs\"\n",
    "                    ][0],\n",
    "                )\n",
    "            }\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "\n",
    "            # Combine all events in the current ROOT file\n",
    "            combined_data_dict = {\n",
    "                \"gen_features\": [],\n",
    "                \"track_features\": [],\n",
    "                \"cluster_features\": [],\n",
    "                \"calo_hit_features\": [],\n",
    "                \"tracker_hit_features\": [],\n",
    "                \"genparticle_to_calo_hit_matrix\": [],\n",
    "                \"genparticle_to_tracker_hit_matrix\": [],\n",
    "                \"track_to_tracker_hit_matrix\": [],\n",
    "                \"cluster_to_cluster_hit_matrix\": [],\n",
    "                \"gp_to_track_matrix\": [],\n",
    "                \"gp_to_gp\": [],\n",
    "            }\n",
    "\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(event_data, iev)\n",
    "                track_features = track_to_features(event_data, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, _ = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, _ = process_tracker_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, _ = create_track_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                cluster_to_cluster_hit_matrix, _ = create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "                # Append the event data to the combined data dictionary\n",
    "                combined_data_dict[\"gen_features\"].append(gen_features)\n",
    "                combined_data_dict[\"track_features\"].append(track_features)\n",
    "                combined_data_dict[\"cluster_features\"].append(cluster_features)\n",
    "                combined_data_dict[\"calo_hit_features\"].append(calo_hit_features)\n",
    "                combined_data_dict[\"tracker_hit_features\"].append(tracker_hit_features)\n",
    "                combined_data_dict[\"genparticle_to_calo_hit_matrix\"].append(genparticle_to_calo_hit_matrix)\n",
    "                combined_data_dict[\"genparticle_to_tracker_hit_matrix\"].append(genparticle_to_tracker_hit_matrix)\n",
    "                combined_data_dict[\"track_to_tracker_hit_matrix\"].append(track_to_tracker_hit_matrix)\n",
    "                combined_data_dict[\"cluster_to_cluster_hit_matrix\"].append(cluster_to_cluster_hit_matrix)\n",
    "                combined_data_dict[\"gp_to_track_matrix\"].append(gp_to_track_matrix)\n",
    "                combined_data_dict[\"gp_to_gp\"].append(gp_to_gp)\n",
    "\n",
    "            # Convert lists to ak arrays\n",
    "            for key in combined_data_dict.keys():\n",
    "                combined_data_dict[key] = ak.Array(combined_data_dict[key])\n",
    "\n",
    "            # Save the combined data into a single Parquet file\n",
    "            ak.to_parquet(combined_data_dict, output_file)\n",
    "\n",
    "            print(f\"Saved combined data for {root_file} to {output_file}\")\n",
    "            root_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    print(f\"Finished processing {root_counter} ROOT files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_root_files_to_parquet(root_file_path, parquet_file_path, max_root_files=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data1 = ak.from_parquet(next(Path(parquet_file_path).glob(\"*.parquet\")))\n",
    "event_data1.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LMDB dataset class. This is currently not used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumps(obj):\n",
    "    \"\"\"\n",
    "    Serialize an object.\n",
    "\n",
    "    Returns:\n",
    "        Implementation-dependent bytes-like object\n",
    "    \"\"\"\n",
    "    return pickle.dumps(obj, protocol=5)\n",
    "\n",
    "\n",
    "def process_root_files_to_lmdb(input_dir, output, max_root_files=None):\n",
    "    \"\"\"\n",
    "    Process ROOT files and save extracted features into an LMDB database.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str or Path): Directory containing ROOT files.\n",
    "        output (str or Path): Path to the LMDB database file.\n",
    "        collectionIDs (dict): Mapping of collection names to their IDs.\n",
    "        max_root_files (int, optional): Maximum number of ROOT files to process. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    lmdb_path = os.path.expanduser(output)\n",
    "    isdir = os.path.isdir(lmdb_path)\n",
    "\n",
    "    print(\"Generate LMDB to %s\" % lmdb_path)\n",
    "    db = lmdb.open(lmdb_path, subdir=isdir, map_size=1099511627776 * 2, readonly=False, meminit=False, map_async=True)\n",
    "\n",
    "    txn = db.begin(write=True)\n",
    "    event_counter = 0\n",
    "    root_counter = 0\n",
    "\n",
    "    for root_file in tqdm(Path(input_dir).rglob(\"*.root\"), desc=\"Processing ROOT files\"):\n",
    "        if max_root_files is not None and root_counter >= max_root_files:\n",
    "            print(f\"Reached max_root_files limit: {max_root_files}. Stopping processing.\")\n",
    "            break\n",
    "        try:\n",
    "            fi = uproot.open(root_file)\n",
    "            collectionIDs = {\n",
    "                k: v\n",
    "                for k, v in zip(\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_names\")[\"events___idTable/m_names\"][0],\n",
    "                    fi.get(\"podio_metadata\").arrays(\"events___idTable/m_collectionIDs\")[\n",
    "                        \"events___idTable/m_collectionIDs\"\n",
    "                    ][0],\n",
    "                )\n",
    "            }\n",
    "            ev = fi[\"events\"]\n",
    "            event_data = get_event_data(ev)\n",
    "            for iev in range(len(ev[\"MCParticles.momentum.x\"].array())):\n",
    "                # Extract features and adjacency matrices for the current event\n",
    "                gen_features = gen_to_features(event_data, iev)\n",
    "                track_features = track_to_features(event_data, iev)\n",
    "                cluster_features = cluster_to_features(\n",
    "                    event_data, iev, cluster_features=[\"position.x\", \"position.y\", \"position.z\", \"energy\", \"type\"]\n",
    "                )\n",
    "                calo_hit_features, genparticle_to_calo_hit_matrix, _ = process_calo_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                tracker_hit_features, genparticle_to_tracker_hit_matrix, _ = process_tracker_hit_data(\n",
    "                    event_data, iev, collectionIDs\n",
    "                )\n",
    "                track_to_tracker_hit_matrix, _ = create_track_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                cluster_to_cluster_hit_matrix, _ = create_cluster_to_hit_coo_matrix(event_data, iev, collectionIDs)\n",
    "                gp_to_track_matrix = genparticle_track_adj(event_data, iev)\n",
    "                gp_to_gp = create_genparticle_to_genparticle_coo_matrix(event_data, iev)\n",
    "\n",
    "                # Save the event data into LMDB\n",
    "                txn.put(\n",
    "                    \"{}\".format(event_counter).encode(\"ascii\"),\n",
    "                    dumps(\n",
    "                        {\n",
    "                            \"gen_features\": gen_features,\n",
    "                            \"track_features\": track_features,\n",
    "                            \"cluster_features\": cluster_features,\n",
    "                            \"calo_hit_features\": calo_hit_features,\n",
    "                            \"tracker_hit_features\": tracker_hit_features,\n",
    "                            \"genparticle_to_calo_hit_matrix\": genparticle_to_calo_hit_matrix,\n",
    "                            \"genparticle_to_tracker_hit_matrix\": genparticle_to_tracker_hit_matrix,\n",
    "                            \"track_to_tracker_hit_matrix\": track_to_tracker_hit_matrix,\n",
    "                            \"cluster_to_cluster_hit_matrix\": cluster_to_cluster_hit_matrix,\n",
    "                            \"gp_to_track_matrix\": gp_to_track_matrix,\n",
    "                            \"gp_to_gp\": gp_to_gp,\n",
    "                        }\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                if event_counter % 100 == 0:\n",
    "                    print(f\"[{event_counter}] events processed\")\n",
    "                    txn.commit()\n",
    "                    txn = db.begin(write=True)\n",
    "\n",
    "                event_counter += 1\n",
    "\n",
    "            root_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root_file}: {e}\")\n",
    "\n",
    "    # Finish iterating through all events\n",
    "    txn.commit()\n",
    "    keys = [\"{}\".format(k).encode(\"ascii\") for k in range(event_counter)]\n",
    "    with db.begin(write=True) as txn:\n",
    "        txn.put(b\"__keys__\", dumps(keys))\n",
    "        txn.put(b\"__len__\", dumps(len(keys)))\n",
    "\n",
    "    print(\"Flushing database ...\")\n",
    "    db.sync()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_root_files_to_lmdb(\n",
    "#     input_dir=\"/mnt/ceph/users/ewulff/data/cld/\", output=\"/mnt/ceph/users/ewulff/data/cld/processed/lmdb\", max_root_files=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lmdb database\n",
    "def read_full_lmdb_database(lmdb_path):\n",
    "    lmdb_path = os.path.expanduser(lmdb_path)\n",
    "    db = lmdb.open(lmdb_path, subdir=os.path.isdir(lmdb_path), readonly=True, lock=False)\n",
    "\n",
    "    with db.begin() as txn:\n",
    "        keys = pickle.loads(txn.get(b\"__keys__\"))\n",
    "        data = {ii: pickle.loads(txn.get(key)) for ii, key in enumerate(keys)}\n",
    "\n",
    "    db.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "# lmdb_data = read_full_lmdb_database(\"/mnt/ceph/users/ewulff/data/cld/processed/lmdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data1 = ak.from_parquet(next(Path(parquet_file_path).glob(\"*.parquet\")))\n",
    "\n",
    "# Extract genparticle_to_calo_hit_matrix from event_data1\n",
    "# This matrix contains the mapping of genparticles to calorimeter hits in a COO format\n",
    "event_i = 7\n",
    "gen_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"gen_idx\"].to_numpy()\n",
    "hit_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"hit_idx\"].to_numpy()\n",
    "weights = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\n",
    "    \"weight\"\n",
    "].to_numpy()  # only contains the non-zero weights\n",
    "\n",
    "# create dense coo amtrix\n",
    "# gp_to_calo_hit_matrix = coo_matrix((weights, (rows, cols)), shape=(np.max(rows) + 1, np.max(cols) + 1)).todense()\n",
    "\n",
    "# Create instance segmentation-like labels for each hit\n",
    "# Each hit is classified as belonging to one genparticle based on the highest weight\n",
    "\n",
    "\n",
    "def get_hit_labels(hit_idx, gen_idx, weights, max_hits=None):\n",
    "    \"\"\"\n",
    "    Assign labels to hits based on the genparticle index with the highest weight.\n",
    "\n",
    "    Parameters:\n",
    "        hit_idx (np.ndarray): Array of hit indices.\n",
    "        gen_idx (np.ndarray): Array of genparticle indices corresponding to each hit.\n",
    "        weights (np.ndarray): Array of weights corresponding to each hit.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of labels for each hit, where each label corresponds to the genparticle index.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store labels for each hit\n",
    "    if not max_hits:\n",
    "        max_hits = np.max(hit_idx) + 1\n",
    "    hit_labels = np.full(max_hits, -1, dtype=int)  # Default label is -1 (unclassified)\n",
    "    hit_label_weights = dict()  # To keep track of the highest weight for each hit\n",
    "\n",
    "    # Iterate through the sparse COO matrix data\n",
    "    for h_idx, g_idx, weight in zip(hit_idx, gen_idx, weights):\n",
    "        if hit_labels[h_idx] == -1 or weight > hit_label_weights[h_idx]:\n",
    "            hit_labels[h_idx] = g_idx\n",
    "            hit_label_weights[h_idx] = weight\n",
    "\n",
    "    # hit_labels now contains the genparticle index for each hit\n",
    "\n",
    "    return hit_labels\n",
    "\n",
    "\n",
    "hit_labels = get_hit_labels(hit_idx, gen_idx, weights)\n",
    "\n",
    "# hit_labels are fewer than gen_idx and hit_idx because some hits are associated to more than one genparticle\n",
    "hit_labels.shape, gen_idx.shape, hit_idx.shape, weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot calorimeter hits colored by genparticle from the root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"gen_idx\"].to_numpy()\n",
    "hit_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"hit_idx\"].to_numpy()\n",
    "weights = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\n",
    "    \"weight\"\n",
    "].to_numpy()  # only contains the non-zero weights\n",
    "calo_hit_features = event_data1[\"calo_hit_features\"][event_i]\n",
    "\n",
    "# Assign labels to hits based on the genparticle index with the highest weight\n",
    "hit_labels = get_hit_labels(hit_idx, gen_idx, weights)\n",
    "\n",
    "# Assign labels based on pandora clusters\n",
    "hit_labels2 = get_hit_labels(\n",
    "    event_data1[\"cluster_to_cluster_hit_matrix\"][event_i][\"hit_idx\"],\n",
    "    event_data1[\"cluster_to_cluster_hit_matrix\"][event_i][\"cluster_idx\"],\n",
    "    event_data1[\"cluster_to_cluster_hit_matrix\"][event_i][\"weight\"],\n",
    "    max_hits = np.max(hit_idx)+1\n",
    ")\n",
    "\n",
    "# Extract calorimeter hit positions (x, y, z)\n",
    "calo_hit_positions = np.column_stack(\n",
    "    (\n",
    "        calo_hit_features[\"position.x\"].to_numpy(),\n",
    "        calo_hit_features[\"position.y\"].to_numpy(),\n",
    "        calo_hit_features[\"position.z\"].to_numpy(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def plot_calo_hits_colored_by_genparticle(hit_labels, calo_hit_positions, title=\"Calorimeter hits colored by genparticle\"):\n",
    "    # Assign unique colors to each genparticle ID\n",
    "    unique_ids = np.unique(hit_labels)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_ids)))\n",
    "    color_map = {\n",
    "        gen_id: f\"rgba({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)}, {color[3]})\"\n",
    "        for gen_id, color in zip(unique_ids, colors)\n",
    "    }\n",
    "\n",
    "    # random color map\n",
    "    def random_color():\n",
    "        \"\"\"Generate a random color in RGBA format.\"\"\"\n",
    "        return f\"rgba({random.randint(0, 255)}, {random.randint(0, 255)}, {random.randint(0, 255)}, 1)\"\n",
    "\n",
    "    random_color_map = {gen_id: random_color() for gen_id in unique_ids}\n",
    "    random_color_map[-1] = \"rgba(0,0,0)\"\n",
    " \n",
    "    # Create traces for each genparticle ID\n",
    "    traces = []\n",
    "    for gen_id in unique_ids:\n",
    "        mask = hit_labels == gen_id  # Create a mask for hits belonging to the current genparticle ID\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x=calo_hit_positions[mask, 0],\n",
    "                y=calo_hit_positions[mask, 1],\n",
    "                z=calo_hit_positions[mask, 2],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, color=random_color_map[gen_id]),\n",
    "                name=f\"gp {gen_id}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Customize the axis names\n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"X\"),\n",
    "            yaxis=dict(title=\"Y\"),\n",
    "            zaxis=dict(title=\"Z\"),\n",
    "            camera=dict(\n",
    "                up=dict(x=1, y=0, z=0),  # Sets the orientation of the camera\n",
    "                center=dict(x=0, y=0, z=0),  # Sets the center point of the plot\n",
    "                eye=dict(x=0, y=0, z=2.1),  # Sets the position of the camera\n",
    "            ),\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        width=700,\n",
    "        height=700,\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    # Create the figure and display the plot\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_calo_hits_colored_by_genparticle(hit_labels, calo_hit_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calo_hits_colored_by_genparticle(hit_labels2, calo_hit_positions, \"Calorimeter hits colored by Pandora cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "This is defined in `src.datasets.CLDHits.py`but reproduced here for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDHits(IterableDataset):\n",
    "    def __init__(self, folder_path, split, shuffle_files=False, train_fraction=0.8):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by storing the paths to all parquet files in the specified folder.\n",
    "\n",
    "        Args:\n",
    "            folder_path (str or Path): Path to the folder containing parquet files.\n",
    "            shuffle_files (bool): Whether to shuffle the order of parquet files.\n",
    "        \"\"\"\n",
    "        self.folder_path = Path(folder_path)\n",
    "        self.parquet_files = list(self.folder_path.glob(\"*.parquet\"))\n",
    "        print(self.parquet_files)\n",
    "        self.shuffle_files = shuffle_files\n",
    "\n",
    "        self.split = split\n",
    "        if self.split is not None:\n",
    "            split_index = int(len(self.parquet_files) * train_fraction)\n",
    "            if self.split == \"train\":\n",
    "                self.parquet_files = self.parquet_files[:split_index]\n",
    "            elif self.split == \"val\":\n",
    "                self.parquet_files = self.parquet_files[split_index:]\n",
    "        print(split_index)\n",
    "\n",
    "        if self.shuffle_files:\n",
    "            self.shuffle_shards()\n",
    "\n",
    "    def shuffle_shards(self):\n",
    "        \"\"\"\n",
    "        Shuffle the parquet files. This can be called at the start of every epoch.\n",
    "        \"\"\"\n",
    "        random.shuffle(self.parquet_files)\n",
    "\n",
    "    def __iter__(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            # Single-process data loading\n",
    "            files_to_process = self.parquet_files\n",
    "            logger.info(f\"Processing {len(files_to_process)} files in single-process mode.\")\n",
    "\n",
    "        else:\n",
    "            # Multi-process data loading, split the files among workers\n",
    "            worker_id = worker_info.id\n",
    "            num_workers = worker_info.num_workers\n",
    "            files_to_process = self.parquet_files[worker_id::num_workers]\n",
    "            logger.info(f\"Processing {len(files_to_process)} files out of {len(self.parquet_files)} total files.\")\n",
    "\n",
    "        for file in files_to_process:\n",
    "            data = ak.from_parquet(file)\n",
    "            for event_i in range(len(data[\"genparticle_to_calo_hit_matrix\"])):\n",
    "                genparticle_to_calo_hit_matrix = data[\"genparticle_to_calo_hit_matrix\"][event_i]\n",
    "                cluster_to_cluster_hit_matrix = data[\"cluster_to_cluster_hit_matrix\"][event_i]\n",
    "                calo_hit_features = data[\"calo_hit_features\"][event_i]\n",
    "\n",
    "                gen_idx = genparticle_to_calo_hit_matrix[\"gen_idx\"].to_numpy()\n",
    "                hit_idx = genparticle_to_calo_hit_matrix[\"hit_idx\"].to_numpy()\n",
    "                weights = genparticle_to_calo_hit_matrix[\"weight\"].to_numpy()\n",
    "\n",
    "                # Extract calorimeter hit positions (x, y, z)\n",
    "                calo_hit_features = np.column_stack(\n",
    "                    (\n",
    "                        calo_hit_features[\"position.x\"].to_numpy(),\n",
    "                        calo_hit_features[\"position.y\"].to_numpy(),\n",
    "                        calo_hit_features[\"position.z\"].to_numpy(),\n",
    "                        calo_hit_features[\"energy\"].to_numpy(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                hit_labels = get_hit_labels(\n",
    "                    hit_idx, gen_idx, weights\n",
    "                )  # This could be moved to the pre-processing step if needed\n",
    "                \n",
    "                hit_labels2 = get_hit_labels(\n",
    "                    cluster_to_cluster_hit_matrix[\"hit_idx\"],\n",
    "                    cluster_to_cluster_hit_matrix[\"cluster_idx\"],\n",
    "                    cluster_to_cluster_hit_matrix[\"weight\"],\n",
    "                    max_hits = np.max(hit_idx)+1\n",
    "                )\n",
    "\n",
    "                yield {\n",
    "                    # \"gen_idx\": gen_idx,\n",
    "                    # \"hit_idx\": hit_idx,\n",
    "                    # \"weights\": weights,\n",
    "                    \"hit_labels\": hit_labels,\n",
    "                    \"hit_labels_pandora\": hit_labels2,\n",
    "                    \"calo_hit_features\": calo_hit_features,\n",
    "                }\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = CLDHits(parquet_file_path, \"train\")\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create the dataset and dataloader with the custom collate function\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    collate_fn=Collater([\"calo_hit_features\", \"hit_labels\", \"hit_labels_pandora\"], []),\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Iterate through batches of events\n",
    "    for batch in dataloader:\n",
    "        print(batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick test of the CLDHits class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data for testing\n",
    "def create_mock_event_data():\n",
    "    return ak.Array(\n",
    "        {\n",
    "            \"genparticle_to_calo_hit_matrix\": [\n",
    "                {\n",
    "                    \"gen_idx\": np.array([0, 1, 0, 2]),\n",
    "                    \"hit_idx\": np.array([0, 1, 2, 3]),\n",
    "                    \"weight\": np.array([0.5, 0.8, 0.3, 0.9]),\n",
    "                },\n",
    "                {\n",
    "                    \"gen_idx\": np.array([1, 2, 1, 0]),\n",
    "                    \"hit_idx\": np.array([0, 1, 2, 3]),\n",
    "                    \"weight\": np.array([0.6, 0.7, 0.4, 0.2]),\n",
    "                },\n",
    "                {\n",
    "                    \"gen_idx\": np.array([0, 2, 1, 2]),\n",
    "                    \"hit_idx\": np.array([0, 1, 2, 3]),\n",
    "                    \"weight\": np.array([0.9, 0.5, 0.7, 0.8]),\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test_hit_labels():\n",
    "    event_data1 = create_mock_event_data()\n",
    "    event_i = 2\n",
    "    gen_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"gen_idx\"].to_numpy()\n",
    "    hit_idx = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"hit_idx\"].to_numpy()\n",
    "    weights = event_data1[\"genparticle_to_calo_hit_matrix\"][event_i][\"weight\"].to_numpy()\n",
    "\n",
    "    hit_labels = get_hit_labels(hit_idx, gen_idx, weights)\n",
    "\n",
    "    # Expected labels based on mock data\n",
    "    expected_labels = np.array([0, 2, 1, 2])\n",
    "\n",
    "    assert np.array_equal(hit_labels, expected_labels), f\"Expected {expected_labels}, but got {hit_labels}\"\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_hit_labels()\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_genparticles(event_data, iev):\n",
    "    \"\"\"\n",
    "    Identify genparticles that leave more than a specified number of hits in the calorimeter.\n",
    "\n",
    "    Args:\n",
    "        gp_to_calo_hit_matrix (numpy.ndarray): The matrix representing the mapping of genparticles to calorimeter hits.\n",
    "        threshold (int): The minimum number of hits a genparticle must leave to be included in the result.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Indices of genparticles that leave more than the specified number of hits.\n",
    "    \"\"\"\n",
    "\n",
    "    gen_idx = event_data[\"genparticle_to_calo_hit_matrix\"][iev][\"gen_idx\"].to_numpy()\n",
    "    hit_idx = event_data[\"genparticle_to_calo_hit_matrix\"][iev][\"hit_idx\"].to_numpy()\n",
    "    weights = event_data[\"genparticle_to_calo_hit_matrix\"][iev][\n",
    "        \"weight\"\n",
    "    ].to_numpy()  # only contains the non-zero weights\n",
    "\n",
    "    # create dense coo amtrix\n",
    "    gp_to_calo_hit_matrix = coo_matrix(\n",
    "        (weights, (gen_idx, hit_idx)), shape=(np.max(gen_idx) + 1, np.max(hit_idx) + 1)\n",
    "    ).todense()\n",
    "\n",
    "    # Extract rows that sum to greater than 0\n",
    "    non_zero_rows = np.where(np.sum(gp_to_calo_hit_matrix, axis=1) > 0)[0]\n",
    "\n",
    "    print(f\"Number of genparticles: {gp_to_calo_hit_matrix.shape[0]}\")\n",
    "    print(f\"Number of genparticles with > 0 hits: {len(non_zero_rows)}\")\n",
    "    print(f\"Number of genparticles with > 1 hits: {np.sum(np.sum(gp_to_calo_hit_matrix > 0, axis=1) > 1)}\")\n",
    "\n",
    "\n",
    "count_genparticles(event_data1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot calorimeter hits colored by genparticle from the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.CLDHits import CLDHits\n",
    "from src.datasets.utils import Collater\n",
    "\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size, ntrain=None, nvalid=None):\n",
    "    train_dataset = CLDHits(data_dir, \"train\", nsamples=ntrain)\n",
    "    val_dataset = CLDHits(data_dir, \"val\", nsamples=nvalid)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=Collater(\"all\"))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=Collater(\"all\"))\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_dl, val_dl = get_dataloaders(\n",
    "    parquet_file_path,\n",
    "    batch_size=1,\n",
    "    ntrain=1000,  # Number of training samples\n",
    "    nvalid=200,  # Number of validation samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dl), len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_dl:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.CLDHits import inverse_standardize_calo_hit_features\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "calo_hit_features = batch[\"calo_hit_features\"]\n",
    "hit_labels = batch[\"hit_labels\"]\n",
    "hit_labels_pandora = batch[\"hit_labels_pandora\"]\n",
    "\n",
    "calo_hit_features = inverse_standardize_calo_hit_features(calo_hit_features)\n",
    "\n",
    "calo_hit_positions = np.column_stack(\n",
    "    (\n",
    "        calo_hit_features[0, :, 0],\n",
    "        calo_hit_features[0, :, 1],\n",
    "        calo_hit_features[0, :, 2],\n",
    "    )\n",
    ")\n",
    "hit_labels = hit_labels.squeeze()\n",
    "hit_labels_pandora = hit_labels_pandora.squeeze()\n",
    "calo_hit_positions.shape, hit_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calo_hits_colored_by_genparticle(hit_labels, calo_hit_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calo_hits_colored_by_genparticle(hit_labels_pandora, calo_hit_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
